#!/usr/bin/env python3
"""
è§†è§‰è¾…åŠ©å†³ç­–æ¨¡å— - åŸºäºå±å¹•åˆ†æçš„æ™ºèƒ½æ¨è
"""
import cv2
import numpy as np
import pyautogui
from PIL import Image, ImageDraw
from typing import Dict, List, Tuple, Optional, NamedTuple
from dataclasses import dataclass
import logging
import pytesseract
import re
from pathlib import Path

logger = logging.getLogger(__name__)

# ç¦ç”¨pyautoguiçš„å®‰å…¨æ£€æŸ¥
pyautogui.FAILSAFE = False


class UIElement(NamedTuple):
    """UIå…ƒç´ å®šä¹‰"""
    type: str  # å…ƒç´ ç±»å‹
    position: Tuple[int, int, int, int]  # (x, y, width, height)
    text: str  # å…ƒç´ æ–‡æœ¬å†…å®¹
    confidence: float  # è¯†åˆ«ç½®ä¿¡åº¦
    actionable: bool  # æ˜¯å¦å¯æ“ä½œ


@dataclass
class VisualContext:
    """è§†è§‰ä¸Šä¸‹æ–‡"""
    active_window: str
    ui_elements: List[UIElement]
    focus_area: Optional[Tuple[int, int, int, int]]
    suggested_actions: List[str]
    screenshot_path: Optional[str] = None


class ScreenCapture:
    """å±å¹•æ•è·å™¨"""
    
    def __init__(self):
        self.last_screenshot = None
        self.screenshot_cache = {}
    
    def capture_screen(self, region: Optional[Tuple[int, int, int, int]] = None) -> Image.Image:
        """
        æ•è·å±å¹•æˆªå›¾
        
        Args:
            region: åŒºåŸŸ (x, y, width, height)ï¼ŒNoneè¡¨ç¤ºå…¨å±
            
        Returns:
            PIL Imageå¯¹è±¡
        """
        try:
            if region:
                screenshot = pyautogui.screenshot(region=region)
            else:
                screenshot = pyautogui.screenshot()
            
            self.last_screenshot = screenshot
            return screenshot
            
        except Exception as e:
            logger.error(f"å±å¹•æ•è·å¤±è´¥: {e}")
            # è¿”å›ç©ºç™½å›¾åƒä½œä¸ºfallback
            return Image.new('RGB', (800, 600), color='white')
    
    def capture_window(self, window_title: str) -> Optional[Image.Image]:
        """æ•è·æŒ‡å®šçª—å£çš„æˆªå›¾"""
        try:
            # ä½¿ç”¨pyautoguiè·å–çª—å£ä¿¡æ¯
            windows = pyautogui.getAllWindows()
            target_window = None
            
            for window in windows:
                if window_title.lower() in window.title.lower():
                    target_window = window
                    break
            
            if target_window:
                # æ¿€æ´»çª—å£
                target_window.activate()
                
                # è·å–çª—å£åŒºåŸŸ
                region = (target_window.left, target_window.top, 
                         target_window.width, target_window.height)
                
                return self.capture_screen(region)
                
        except Exception as e:
            logger.warning(f"çª—å£æ•è·å¤±è´¥ {window_title}: {e}")
        
        return None


class UIElementDetector:
    """UIå…ƒç´ æ£€æµ‹å™¨"""
    
    def __init__(self):
        # é¢„å®šä¹‰çš„UIå…ƒç´ æ¨¡æ¿
        self.element_templates = {
            'input_field': self._detect_input_fields,
            'button': self._detect_buttons,
            'text_area': self._detect_text_areas,
            'dropdown': self._detect_dropdowns,
            'chat_bubble': self._detect_chat_bubbles
        }
    
    def detect_elements(self, image: Image.Image) -> List[UIElement]:
        """æ£€æµ‹å›¾åƒä¸­çš„UIå…ƒç´ """
        elements = []
        
        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        for element_type, detector_func in self.element_templates.items():
            try:
                detected = detector_func(cv_image)
                elements.extend(detected)
            except Exception as e:
                logger.warning(f"{element_type}æ£€æµ‹å¤±è´¥: {e}")
        
        return elements
    
    def _detect_input_fields(self, image: np.ndarray) -> List[UIElement]:
        """æ£€æµ‹è¾“å…¥æ¡†"""
        elements = []
        
        # è½¬æ¢ä¸ºç°åº¦å›¾
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # æ£€æµ‹çŸ©å½¢è½®å»“ï¼ˆå¯èƒ½æ˜¯è¾“å…¥æ¡†ï¼‰
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            # è®¡ç®—è½®å»“çš„è¾¹ç•ŒçŸ©å½¢
            x, y, w, h = cv2.boundingRect(contour)
            
            # è¿‡æ»¤æ‰å¤ªå°æˆ–å½¢çŠ¶ä¸åˆé€‚çš„åŒºåŸŸ
            if w > 100 and h > 20 and w/h > 3:  # è¾“å…¥æ¡†é€šå¸¸æ˜¯é•¿æ–¹å½¢
                # æå–åŒºåŸŸæ–‡æœ¬
                roi = gray[y:y+h, x:x+w]
                try:
                    text = pytesseract.image_to_string(roi, lang='chi_sim+eng').strip()
                except:
                    text = ""
                
                elements.append(UIElement(
                    type='input_field',
                    position=(x, y, w, h),
                    text=text,
                    confidence=0.7,
                    actionable=True
                ))
        
        return elements
    
    def _detect_buttons(self, image: np.ndarray) -> List[UIElement]:
        """æ£€æµ‹æŒ‰é’®"""
        elements = []
        
        # ä½¿ç”¨æ¨¡æ¿åŒ¹é…æ£€æµ‹å¸¸è§æŒ‰é’®
        button_texts = ['ç¡®å®š', 'å–æ¶ˆ', 'æäº¤', 'å‘é€', 'æœç´¢', 'ç™»å½•', 'æ³¨å†Œ']
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ç®€åŒ–ç‰ˆæŒ‰é’®æ£€æµ‹ï¼šå¯»æ‰¾åŒ…å«æŒ‰é’®æ–‡å­—çš„åŒºåŸŸ
        try:
            # ä½¿ç”¨OCRæ£€æµ‹æ‰€æœ‰æ–‡æœ¬
            data = pytesseract.image_to_data(gray, lang='chi_sim+eng', output_type=pytesseract.Output.DICT)
            
            for i, text in enumerate(data['text']):
                if text.strip() in button_texts:
                    x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
                    
                    elements.append(UIElement(
                        type='button',
                        position=(x, y, w, h),
                        text=text.strip(),
                        confidence=0.8,
                        actionable=True
                    ))
        except Exception as e:
            logger.warning(f"æŒ‰é’®æ£€æµ‹OCRå¤±è´¥: {e}")
        
        return elements
    
    def _detect_text_areas(self, image: np.ndarray) -> List[UIElement]:
        """æ£€æµ‹æ–‡æœ¬åŒºåŸŸ"""
        elements = []
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # æ£€æµ‹å¤§å—æ–‡æœ¬åŒºåŸŸ
        try:
            data = pytesseract.image_to_data(gray, lang='chi_sim+eng', output_type=pytesseract.Output.DICT)
            
            # åˆå¹¶ç›¸é‚»çš„æ–‡æœ¬å—
            text_blocks = []
            current_block = None
            
            for i, text in enumerate(data['text']):
                if text.strip():
                    x, y, w, h = data['left'][i], data['top'][i], data['width'][i], data['height'][i]
                    
                    if current_block is None:
                        current_block = {'x': x, 'y': y, 'w': w, 'h': h, 'text': text}
                    else:
                        # å¦‚æœæ–‡æœ¬å—ç›¸é‚»ï¼Œåˆå¹¶
                        if abs(y - current_block['y']) < 30:  # åŒä¸€è¡Œæˆ–ç›¸é‚»è¡Œ
                            current_block['w'] = max(x + w - current_block['x'], current_block['w'])
                            current_block['h'] = max(y + h - current_block['y'], current_block['h'])
                            current_block['text'] += ' ' + text
                        else:
                            # ä¿å­˜å½“å‰å—ï¼Œå¼€å§‹æ–°å—
                            if len(current_block['text']) > 10:  # è¿‡æ»¤çŸ­æ–‡æœ¬
                                text_blocks.append(current_block)
                            current_block = {'x': x, 'y': y, 'w': w, 'h': h, 'text': text}
            
            # æ·»åŠ æœ€åä¸€ä¸ªå—
            if current_block and len(current_block['text']) > 10:
                text_blocks.append(current_block)
            
            # è½¬æ¢ä¸ºUIElement
            for block in text_blocks:
                elements.append(UIElement(
                    type='text_area',
                    position=(block['x'], block['y'], block['w'], block['h']),
                    text=block['text'],
                    confidence=0.6,
                    actionable=False
                ))
                
        except Exception as e:
            logger.warning(f"æ–‡æœ¬åŒºåŸŸæ£€æµ‹å¤±è´¥: {e}")
        
        return elements
    
    def _detect_dropdowns(self, image: np.ndarray) -> List[UIElement]:
        """æ£€æµ‹ä¸‹æ‹‰èœå•"""
        # ç®€åŒ–å®ç°ï¼šæ£€æµ‹å¸¦æœ‰ä¸‹æ‹‰ç®­å¤´çš„åŒºåŸŸ
        return []
    
    def _detect_chat_bubbles(self, image: np.ndarray) -> List[UIElement]:
        """æ£€æµ‹èŠå¤©æ°”æ³¡"""
        elements = []
        
        # æ£€æµ‹åœ†è§’çŸ©å½¢ï¼ˆèŠå¤©æ°”æ³¡çš„ç‰¹å¾ï¼‰
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨å½¢æ€å­¦æ“ä½œæ£€æµ‹æ°”æ³¡å½¢çŠ¶
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20, 20))
        morph = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)
        
        contours, _ = cv2.findContours(morph, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            
            # è¿‡æ»¤åˆé€‚å¤§å°çš„åŒºåŸŸ
            if 50 < w < 400 and 30 < h < 200:
                # æå–æ–‡æœ¬
                roi = gray[y:y+h, x:x+w]
                try:
                    text = pytesseract.image_to_string(roi, lang='chi_sim+eng').strip()
                    if text:  # åªæœ‰åŒ…å«æ–‡æœ¬çš„æ‰è®¤ä¸ºæ˜¯èŠå¤©æ°”æ³¡
                        elements.append(UIElement(
                            type='chat_bubble',
                            position=(x, y, w, h),
                            text=text,
                            confidence=0.5,
                            actionable=False
                        ))
                except:
                    pass
        
        return elements


class FocusAreaDetector:
    """ç„¦ç‚¹åŒºåŸŸæ£€æµ‹å™¨"""
    
    def detect_focus(self, image: Image.Image, elements: List[UIElement]) -> Optional[Tuple[int, int, int, int]]:
        """
        æ£€æµ‹ç”¨æˆ·å…³æ³¨åŒºåŸŸ
        
        ç®€åŒ–ç‰ˆå®ç°ï¼šè¿”å›æœ€å¯èƒ½çš„è¾“å…¥åŒºåŸŸ
        """
        # ä¼˜å…ˆçº§ï¼šè¾“å…¥æ¡† > æŒ‰é’® > å…¶ä»–
        priority_order = ['input_field', 'button', 'text_area']
        
        for element_type in priority_order:
            for element in elements:
                if element.type == element_type and element.actionable:
                    return element.position
        
        return None


class ActionSuggester:
    """åŠ¨ä½œå»ºè®®å™¨"""
    
    def suggest_actions(self, context: VisualContext) -> List[str]:
        """æ ¹æ®è§†è§‰ä¸Šä¸‹æ–‡å»ºè®®åŠ¨ä½œ"""
        actions = []
        
        # æ ¹æ®æ£€æµ‹åˆ°çš„å…ƒç´ ç±»å‹å»ºè®®åŠ¨ä½œ
        element_types = [elem.type for elem in context.ui_elements]
        
        if 'input_field' in element_types:
            actions.extend(['è‡ªåŠ¨å¡«å†™', 'æ™ºèƒ½è¾“å…¥å»ºè®®', 'è¡¨å•éªŒè¯'])
        
        if 'button' in element_types:
            actions.extend(['ä¸€é”®æäº¤', 'æ‰¹é‡æ“ä½œ'])
        
        if 'chat_bubble' in element_types:
            actions.extend(['å¿«é€Ÿå›å¤', 'æ¶ˆæ¯åˆ†æ', 'æƒ…æ„Ÿè¯†åˆ«'])
        
        # æ ¹æ®çª—å£ç±»å‹å»ºè®®ç‰¹å®šåŠ¨ä½œ
        window_lower = context.active_window.lower()
        if 'å¾®ä¿¡' in window_lower or 'wechat' in window_lower:
            actions.extend(['æ¶ˆæ¯æ¨¡æ¿', 'è‡ªåŠ¨å›å¤'])
        elif 'qq' in window_lower:
            actions.extend(['è¡¨æƒ…åŒ…æ¨è', 'ç¾¤èŠåŠ©æ‰‹'])
        elif 'telegram' in window_lower:
            actions.extend(['é¢‘é“ç®¡ç†', 'æœºå™¨äººæŒ‡ä»¤'])
        
        return list(set(actions))  # å»é‡


class VisualAssistant:
    """è§†è§‰è¾…åŠ©å†³ç­–ä¸»ç±»"""
    
    def __init__(self):
        self.screen_capture = ScreenCapture()
        self.element_detector = UIElementDetector()
        self.focus_detector = FocusAreaDetector()
        self.action_suggester = ActionSuggester()
    
    def analyze_context(self, window_title: Optional[str] = None) -> VisualContext:
        """åˆ†æå½“å‰è§†è§‰ä¸Šä¸‹æ–‡"""
        try:
            # æ•è·å±å¹•
            if window_title:
                screenshot = self.screen_capture.capture_window(window_title)
                active_window = window_title
            else:
                screenshot = self.screen_capture.capture_screen()
                active_window = self._get_active_window_title()
            
            if screenshot is None:
                screenshot = self.screen_capture.capture_screen()
                active_window = "æœªçŸ¥çª—å£"
            
            # æ£€æµ‹UIå…ƒç´ 
            ui_elements = self.element_detector.detect_elements(screenshot)
            
            # æ£€æµ‹ç„¦ç‚¹åŒºåŸŸ
            focus_area = self.focus_detector.detect_focus(screenshot, ui_elements)
            
            # åˆ›å»ºä¸Šä¸‹æ–‡å¯¹è±¡
            context = VisualContext(
                active_window=active_window,
                ui_elements=ui_elements,
                focus_area=focus_area,
                suggested_actions=[]
            )
            
            # å»ºè®®åŠ¨ä½œ
            context.suggested_actions = self.action_suggester.suggest_actions(context)
            
            return context
            
        except Exception as e:
            logger.error(f"è§†è§‰ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {e}")
            return VisualContext(
                active_window="é”™è¯¯",
                ui_elements=[],
                focus_area=None,
                suggested_actions=[]
            )
    
    def _get_active_window_title(self) -> str:
        """è·å–å½“å‰æ´»åŠ¨çª—å£æ ‡é¢˜"""
        try:
            active_window = pyautogui.getActiveWindow()
            return active_window.title if active_window else "æœªçŸ¥çª—å£"
        except:
            return "æœªçŸ¥çª—å£"
    
    def save_annotated_screenshot(self, context: VisualContext, output_path: str):
        """ä¿å­˜å¸¦æ ‡æ³¨çš„æˆªå›¾"""
        try:
            screenshot = self.screen_capture.last_screenshot
            if screenshot is None:
                return
            
            # åˆ›å»ºç»˜å›¾å¯¹è±¡
            draw = ImageDraw.Draw(screenshot)
            
            # ç»˜åˆ¶æ£€æµ‹åˆ°çš„å…ƒç´ 
            colors = {
                'input_field': 'red',
                'button': 'green', 
                'text_area': 'blue',
                'chat_bubble': 'orange'
            }
            
            for element in context.ui_elements:
                x, y, w, h = element.position
                color = colors.get(element.type, 'gray')
                
                # ç»˜åˆ¶è¾¹æ¡†
                draw.rectangle([x, y, x+w, y+h], outline=color, width=2)
                
                # æ·»åŠ æ ‡ç­¾
                draw.text((x, y-15), f"{element.type}: {element.text[:20]}", fill=color)
            
            # ç»˜åˆ¶ç„¦ç‚¹åŒºåŸŸ
            if context.focus_area:
                x, y, w, h = context.focus_area
                draw.rectangle([x, y, x+w, y+h], outline='purple', width=3)
                draw.text((x, y-30), "FOCUS", fill='purple')
            
            # ä¿å­˜
            screenshot.save(output_path)
            logger.info(f"æ ‡æ³¨æˆªå›¾å·²ä¿å­˜: {output_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ ‡æ³¨æˆªå›¾å¤±è´¥: {e}")


def main():
    """æµ‹è¯•å…¥å£"""
    assistant = VisualAssistant()
    
    print("ğŸ‘ï¸ è§†è§‰è¾…åŠ©å†³ç­–æ¨¡å—æµ‹è¯•")
    print("=" * 50)
    
    # åˆ†æå½“å‰å±å¹•
    context = assistant.analyze_context()
    
    print(f"æ´»åŠ¨çª—å£: {context.active_window}")
    print(f"æ£€æµ‹åˆ° {len(context.ui_elements)} ä¸ªUIå…ƒç´ :")
    
    for i, element in enumerate(context.ui_elements, 1):
        print(f"  {i}. {element.type} - {element.text[:30]}...")
    
    print(f"\nç„¦ç‚¹åŒºåŸŸ: {context.focus_area}")
    print(f"å»ºè®®åŠ¨ä½œ: {context.suggested_actions}")
    
    # ä¿å­˜æ ‡æ³¨æˆªå›¾
    output_path = "visual_analysis_result.png"
    assistant.save_annotated_screenshot(context, output_path)
    print(f"\næ ‡æ³¨æˆªå›¾å·²ä¿å­˜: {output_path}")


if __name__ == "__main__":
    main()

